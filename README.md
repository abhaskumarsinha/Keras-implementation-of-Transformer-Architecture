# Keras-implementation-of-Transformer-Architecture
This repository presents a Python-based implementation of the Transformer architecture, as proposed by Vaswani et al. in their 2017 paper "Attention is all you need." The implementation is a variant of the original model, featuring a bi-directional design similar to BERT and the ability to predict the right-most token in sequence-to-sequence tasks, similar to GPT. The model was built from scratch using the TensorFlow Keras library. This implementation can be utilized for various natural language processing tasks. The Neural Machine Translation task - from English to Hindi has been demonstrated.
