{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7PD7fTV2Ny_",
        "outputId": "c559c10d-ff21-41aa-b7e9-0f3bbeff3959"
      },
      "id": "M7PD7fTV2Ny_",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz2vY_5A2Zia",
        "outputId": "a9f2b3d3-3f51-4472-e849-c07fb26f3104"
      },
      "id": "Yz2vY_5A2Zia",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f4557399",
      "metadata": {
        "id": "f4557399"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from encoders.apply_bpe import BPE\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "52f73ae1",
      "metadata": {
        "id": "52f73ae1"
      },
      "outputs": [],
      "source": [
        "#from dataset import *\n",
        "from Transformer import *\n",
        "from dataset import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e450d7c5",
      "metadata": {
        "id": "e450d7c5"
      },
      "outputs": [],
      "source": [
        "vocab_size = 5374 + 2\n",
        "embedding_size = 64\n",
        "d_k = 8\n",
        "words = 64\n",
        "total_lines = 100\n",
        "codec_file = open(\"datasets/english_codec.txt\", encoding = 'utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bbfe9049",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbfe9049",
        "outputId": "5eaad674-52b1-4a4a-a472-c8f950faf191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primary Language Tokens: 5195\n",
            "Secondary Language Tokens: 5374\n"
          ]
        }
      ],
      "source": [
        "dataset = Dataset(\"datasets/english_dataset.txt\", \"datasets/hindi_dataset.txt\", total_lines = 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b1c4f944",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1c4f944",
        "outputId": "08c3248b-9fce-43e1-9ad9-131cef5c3afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 10243.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Lines added: 144315\n"
          ]
        }
      ],
      "source": [
        "dataset.create_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3c7e2734",
      "metadata": {
        "id": "3c7e2734"
      },
      "outputs": [],
      "source": [
        "training_data = tf.data.Dataset.from_tensor_slices(((dataset.encoder_inputs, dataset.decoder_inputs), dataset.output_vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "71832575",
      "metadata": {
        "id": "71832575"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    encoder_input = tf.keras.layers.Input((words))\n",
        "    decoder_input = tf.keras.layers.Input((words))\n",
        "    \n",
        "    encoder_embeddings = tf.keras.layers.Embedding(vocab_size + 2, embedding_size)(encoder_input)\n",
        "    decoder_embeddings = tf.keras.layers.Embedding(vocab_size + 2, embedding_size)(decoder_input)\n",
        "    \n",
        "    enc_pos = PositionalEmbedding(embedding_size = embedding_size, words = words)(encoder_embeddings)\n",
        "    dec_pos = PositionalEmbedding(embedding_size = embedding_size, words = words)(decoder_embeddings)\n",
        "    \n",
        "    encoder_outputs1 = Encoder(d_k = d_k, model_embedding = embedding_size)(enc_pos)\n",
        "    decoder_outputs1 = Decoder(input_words = words, model_embedding = embedding_size, d_k = d_k)((dec_pos, encoder_outputs1[0], encoder_outputs1[1]))\n",
        "\n",
        "    encoder_outputs2 = Encoder(d_k = d_k, model_embedding = embedding_size)(encoder_outputs1[1])\n",
        "    decoder_outputs2 = Decoder(input_words = words, model_embedding = embedding_size, d_k = d_k)((decoder_outputs1, encoder_outputs2[0], encoder_outputs2[1]))\n",
        "\n",
        "    encoder_outputs3 = Encoder(d_k = d_k, model_embedding = embedding_size)(encoder_outputs2[1])\n",
        "    decoder_outputs3 = Decoder(input_words = words, model_embedding = embedding_size, d_k = d_k)((decoder_outputs2, encoder_outputs3[0], encoder_outputs3[1]))\n",
        "\n",
        "    encoder_outputs4 = Encoder(d_k = d_k, model_embedding = embedding_size)(encoder_outputs3[1])\n",
        "    decoder_outputs4 = Decoder(input_words = words, model_embedding = embedding_size, d_k = d_k)((decoder_outputs3, encoder_outputs4[0], encoder_outputs4[1]))\n",
        "\n",
        "    encoder_outputs5 = Encoder(d_k = d_k, model_embedding = embedding_size)(encoder_outputs4[1])\n",
        "    decoder_outputs5 = Decoder(input_words = words, model_embedding = embedding_size, d_k = d_k)((decoder_outputs4, encoder_outputs5[0], encoder_outputs5[1]))\n",
        "\n",
        "    encoder_outputs6 = Encoder(d_k = d_k, model_embedding = embedding_size)(encoder_outputs5[1])\n",
        "    decoder_outputs6 = Decoder(input_words = words, model_embedding = embedding_size, d_k = d_k)((decoder_outputs5, encoder_outputs6[0], encoder_outputs6[1]))\n",
        "\n",
        "    encoder_outputs7 = Encoder(d_k = d_k, model_embedding = embedding_size)(encoder_outputs6[1])\n",
        "    decoder_outputs7 = Decoder(input_words = words, model_embedding = embedding_size, d_k = d_k)((decoder_outputs6, encoder_outputs7[0], encoder_outputs7[1]))\n",
        "    \n",
        "    \n",
        "    decoder_outputs = tf.keras.layers.Flatten()(decoder_outputs7)\n",
        "    decoder_outputs = tf.keras.layers.Dense(vocab_size)(decoder_outputs)\n",
        "    \n",
        "    output = tf.nn.softmax(decoder_outputs)\n",
        "    \n",
        "    model = tf.keras.Model(inputs = [encoder_input, decoder_input], outputs=[output])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f7e50544",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7e50544",
        "outputId": "8ba53264-e059-4e3a-dd39-c69d747a2fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 64, 64)\n",
            "(None, 64, 64)\n",
            "(None, 64, 64)\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(None, 64, 64)\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(None, 64, 64)\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(None, 64, 64)\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(None, 64, 64)\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(None, 64, 64)\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(None, 64, 64)\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 64)]         0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 64)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 64, 64)       344192      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 64, 64)       344192      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " positional_embedding (Position  (None, 64, 64)      0           ['embedding[0][0]']              \n",
            " alEmbedding)                                                                                     \n",
            "                                                                                                  \n",
            " positional_embedding_1 (Positi  (None, 64, 64)      0           ['embedding_1[0][0]']            \n",
            " onalEmbedding)                                                                                   \n",
            "                                                                                                  \n",
            " encoder (Encoder)              ((None, 64, 64),     141250      ['positional_embedding[0][0]']   \n",
            "                                 (None, 64, 64))                                                  \n",
            "                                                                                                  \n",
            " decoder (Decoder)              (None, 64, 64)       274051      ['positional_embedding_1[0][0]', \n",
            "                                                                  'encoder[0][0]',                \n",
            "                                                                  'encoder[0][1]']                \n",
            "                                                                                                  \n",
            " encoder_1 (Encoder)            ((None, 64, 64),     141250      ['encoder[0][1]']                \n",
            "                                 (None, 64, 64))                                                  \n",
            "                                                                                                  \n",
            " decoder_1 (Decoder)            (None, 64, 64)       274051      ['decoder[0][0]',                \n",
            "                                                                  'encoder_1[0][0]',              \n",
            "                                                                  'encoder_1[0][1]']              \n",
            "                                                                                                  \n",
            " encoder_2 (Encoder)            ((None, 64, 64),     141250      ['encoder_1[0][1]']              \n",
            "                                 (None, 64, 64))                                                  \n",
            "                                                                                                  \n",
            " decoder_2 (Decoder)            (None, 64, 64)       274051      ['decoder_1[0][0]',              \n",
            "                                                                  'encoder_2[0][0]',              \n",
            "                                                                  'encoder_2[0][1]']              \n",
            "                                                                                                  \n",
            " encoder_3 (Encoder)            ((None, 64, 64),     141250      ['encoder_2[0][1]']              \n",
            "                                 (None, 64, 64))                                                  \n",
            "                                                                                                  \n",
            " decoder_3 (Decoder)            (None, 64, 64)       274051      ['decoder_2[0][0]',              \n",
            "                                                                  'encoder_3[0][0]',              \n",
            "                                                                  'encoder_3[0][1]']              \n",
            "                                                                                                  \n",
            " encoder_4 (Encoder)            ((None, 64, 64),     141250      ['encoder_3[0][1]']              \n",
            "                                 (None, 64, 64))                                                  \n",
            "                                                                                                  \n",
            " decoder_4 (Decoder)            (None, 64, 64)       274051      ['decoder_3[0][0]',              \n",
            "                                                                  'encoder_4[0][0]',              \n",
            "                                                                  'encoder_4[0][1]']              \n",
            "                                                                                                  \n",
            " encoder_5 (Encoder)            ((None, 64, 64),     141250      ['encoder_4[0][1]']              \n",
            "                                 (None, 64, 64))                                                  \n",
            "                                                                                                  \n",
            " decoder_5 (Decoder)            (None, 64, 64)       274051      ['decoder_4[0][0]',              \n",
            "                                                                  'encoder_5[0][0]',              \n",
            "                                                                  'encoder_5[0][1]']              \n",
            "                                                                                                  \n",
            " encoder_6 (Encoder)            ((None, 64, 64),     141250      ['encoder_5[0][1]']              \n",
            "                                 (None, 64, 64))                                                  \n",
            "                                                                                                  \n",
            " decoder_6 (Decoder)            (None, 64, 64)       274051      ['decoder_5[0][0]',              \n",
            "                                                                  'encoder_6[0][0]',              \n",
            "                                                                  'encoder_6[0][1]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 4096)         0           ['decoder_6[0][0]']              \n",
            "                                                                                                  \n",
            " dense_49 (Dense)               (None, 5376)         22025472    ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " tf.nn.softmax (TFOpLambda)     (None, 5376)         0           ['dense_49[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,620,963\n",
            "Trainable params: 25,616,448\n",
            "Non-trainable params: 4,515\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e35ee094",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e35ee094",
        "outputId": "b1f0b670-f1d7-488d-d836-4b7417e2abd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]\n"
          ]
        }
      ],
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "508e074f",
      "metadata": {
        "id": "508e074f"
      },
      "outputs": [],
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fe9823d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe9823d4",
        "outputId": "3fcbb9b6-d90d-4521-ac49-0e19c4ba0ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 64, 64)\n",
            "(None, 64, 64)\n",
            "(None, 64, 64)\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(None, 64, 64)\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(None, 64, 64)\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(None, 64, 64)\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(None, 64, 64)\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(None, 64, 64)\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(None, 64, 64)\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "(TensorShape([None, 64, 64]), TensorShape([None, 64, 64]), TensorShape([None, 64, 64]))\n",
            "Epoch 1/10\n",
            "141/141 [==============================] - 230s 389ms/step - loss: 7.5549\n",
            "Epoch 2/10\n",
            "141/141 [==============================] - 50s 358ms/step - loss: 7.1675\n",
            "Epoch 3/10\n",
            "141/141 [==============================] - 55s 394ms/step - loss: 6.5109\n",
            "Epoch 4/10\n",
            "141/141 [==============================] - 48s 340ms/step - loss: 5.5866\n",
            "Epoch 5/10\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 5.0662\n",
            "Epoch 6/10\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 4.5805\n",
            "Epoch 7/10\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 4.2016\n",
            "Epoch 8/10\n",
            "141/141 [==============================] - 25s 176ms/step - loss: 3.8816\n",
            "Epoch 9/10\n",
            "141/141 [==============================] - 27s 189ms/step - loss: 3.6068\n",
            "Epoch 10/10\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 3.3749\n",
            "Epoch 1/150\n",
            "141/141 [==============================] - 198s 177ms/step - loss: 2.4008\n",
            "Epoch 2/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 2.1602\n",
            "Epoch 3/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 2.0235\n",
            "Epoch 4/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 1.9268\n",
            "Epoch 5/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 1.8506\n",
            "Epoch 6/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 1.7874\n",
            "Epoch 7/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 1.7297\n",
            "Epoch 8/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 1.6770\n",
            "Epoch 9/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 1.6275\n",
            "Epoch 10/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 1.5813\n",
            "Epoch 11/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 1.5358\n",
            "Epoch 12/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 1.4923\n",
            "Epoch 13/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 1.4498\n",
            "Epoch 14/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 1.4124\n",
            "Epoch 15/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 1.3731\n",
            "Epoch 16/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 1.3348\n",
            "Epoch 17/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 1.2990\n",
            "Epoch 18/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 1.2660\n",
            "Epoch 19/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 1.2308\n",
            "Epoch 20/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 1.2018\n",
            "Epoch 21/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 1.1686\n",
            "Epoch 22/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 1.1381\n",
            "Epoch 23/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 1.1107\n",
            "Epoch 24/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 1.0805\n",
            "Epoch 25/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 1.0541\n",
            "Epoch 26/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 1.0249\n",
            "Epoch 27/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.9962\n",
            "Epoch 28/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.9722\n",
            "Epoch 29/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.9439\n",
            "Epoch 30/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.9473\n",
            "Epoch 31/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.9012\n",
            "Epoch 32/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.8815\n",
            "Epoch 33/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.8499\n",
            "Epoch 34/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.8284\n",
            "Epoch 35/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.8177\n",
            "Epoch 36/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.7870\n",
            "Epoch 37/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.7673\n",
            "Epoch 38/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.7524\n",
            "Epoch 39/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.7332\n",
            "Epoch 40/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.7116\n",
            "Epoch 41/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.7033\n",
            "Epoch 42/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.6934\n",
            "Epoch 43/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.6726\n",
            "Epoch 44/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.6567\n",
            "Epoch 45/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.6418\n",
            "Epoch 46/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.6149\n",
            "Epoch 47/150\n",
            "141/141 [==============================] - 25s 180ms/step - loss: 0.6069\n",
            "Epoch 48/150\n",
            "141/141 [==============================] - 25s 180ms/step - loss: 0.5970\n",
            "Epoch 49/150\n",
            "141/141 [==============================] - 25s 180ms/step - loss: 0.5825\n",
            "Epoch 50/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.5630\n",
            "Epoch 51/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.5530\n",
            "Epoch 52/150\n",
            "141/141 [==============================] - 25s 180ms/step - loss: 0.5384\n",
            "Epoch 53/150\n",
            "141/141 [==============================] - 26s 181ms/step - loss: 0.5201\n",
            "Epoch 54/150\n",
            "141/141 [==============================] - 25s 181ms/step - loss: 0.5152\n",
            "Epoch 55/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.5005\n",
            "Epoch 56/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.4816\n",
            "Epoch 57/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.4662\n",
            "Epoch 58/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.4894\n",
            "Epoch 59/150\n",
            "141/141 [==============================] - 25s 180ms/step - loss: 0.4520\n",
            "Epoch 60/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.4664\n",
            "Epoch 61/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.4502\n",
            "Epoch 62/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.4191\n",
            "Epoch 63/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.4008\n",
            "Epoch 64/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.3899\n",
            "Epoch 65/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.3922\n",
            "Epoch 66/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.3956\n",
            "Epoch 67/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.3903\n",
            "Epoch 68/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.3811\n",
            "Epoch 69/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.3571\n",
            "Epoch 70/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.3442\n",
            "Epoch 71/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.3483\n",
            "Epoch 72/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.3363\n",
            "Epoch 73/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.3385\n",
            "Epoch 74/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.3567\n",
            "Epoch 75/150\n",
            "141/141 [==============================] - 25s 181ms/step - loss: 0.3103\n",
            "Epoch 76/150\n",
            "141/141 [==============================] - 25s 181ms/step - loss: 0.2992\n",
            "Epoch 77/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.2888\n",
            "Epoch 78/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.2930\n",
            "Epoch 79/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.3162\n",
            "Epoch 80/150\n",
            "141/141 [==============================] - 26s 184ms/step - loss: 0.2907\n",
            "Epoch 81/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.2707\n",
            "Epoch 82/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.2596\n",
            "Epoch 83/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.2658\n",
            "Epoch 84/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.2517\n",
            "Epoch 85/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.2468\n",
            "Epoch 86/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.2734\n",
            "Epoch 87/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.2500\n",
            "Epoch 88/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.2298\n",
            "Epoch 89/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.2501\n",
            "Epoch 90/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.2650\n",
            "Epoch 91/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.2258\n",
            "Epoch 92/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.2056\n",
            "Epoch 93/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1926\n",
            "Epoch 94/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1895\n",
            "Epoch 95/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.2165\n",
            "Epoch 96/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.2067\n",
            "Epoch 97/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.2242\n",
            "Epoch 98/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.2196\n",
            "Epoch 99/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1860\n",
            "Epoch 100/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.1878\n",
            "Epoch 101/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.1783\n",
            "Epoch 102/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1595\n",
            "Epoch 103/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1799\n",
            "Epoch 104/150\n",
            "141/141 [==============================] - 25s 181ms/step - loss: 0.2303\n",
            "Epoch 105/150\n",
            "141/141 [==============================] - 25s 180ms/step - loss: 0.1638\n",
            "Epoch 106/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.1713\n",
            "Epoch 107/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1530\n",
            "Epoch 108/150\n",
            "141/141 [==============================] - 25s 180ms/step - loss: 0.1516\n",
            "Epoch 109/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.1516\n",
            "Epoch 110/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1804\n",
            "Epoch 111/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1568\n",
            "Epoch 112/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1465\n",
            "Epoch 113/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1366\n",
            "Epoch 114/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.2411\n",
            "Epoch 115/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.1801\n",
            "Epoch 116/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.1527\n",
            "Epoch 117/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.1314\n",
            "Epoch 118/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1462\n",
            "Epoch 119/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1769\n",
            "Epoch 120/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1473\n",
            "Epoch 121/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.2230\n",
            "Epoch 122/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1190\n",
            "Epoch 123/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1067\n",
            "Epoch 124/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.0926\n",
            "Epoch 125/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.0808\n",
            "Epoch 126/150\n",
            "141/141 [==============================] - 25s 179ms/step - loss: 0.0808\n",
            "Epoch 127/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.0851\n",
            "Epoch 128/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.1122\n",
            "Epoch 129/150\n",
            "141/141 [==============================] - 26s 182ms/step - loss: 0.1546\n",
            "Epoch 130/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1662\n",
            "Epoch 131/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.1439\n",
            "Epoch 132/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1056\n",
            "Epoch 133/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.0871\n",
            "Epoch 134/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.0831\n",
            "Epoch 135/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.0979\n",
            "Epoch 136/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.0961\n",
            "Epoch 137/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.1106\n",
            "Epoch 138/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1011\n",
            "Epoch 139/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.1006\n",
            "Epoch 140/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.1074\n",
            "Epoch 141/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.0964\n",
            "Epoch 142/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.0953\n",
            "Epoch 143/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.0894\n",
            "Epoch 144/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1338\n",
            "Epoch 145/150\n",
            "141/141 [==============================] - 25s 177ms/step - loss: 0.1556\n",
            "Epoch 146/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1492\n",
            "Epoch 147/150\n",
            "141/141 [==============================] - 25s 180ms/step - loss: 0.0888\n",
            "Epoch 148/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.0673\n",
            "Epoch 149/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.1081\n",
            "Epoch 150/150\n",
            "141/141 [==============================] - 25s 178ms/step - loss: 0.0810\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    model = create_model()\n",
        "    \n",
        "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1 = 0.01))\n",
        "    \n",
        "    history = model.fit((dataset.encoder_inputs, dataset.decoder_inputs), dataset.output_vectors, batch_size = 1024, epochs = 10)\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001, beta_1 = 0.001))\n",
        "\n",
        "    history = model.fit((dataset.encoder_inputs, dataset.decoder_inputs), dataset.output_vectors, batch_size = 1024, epochs = 150)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c233ad0a",
      "metadata": {
        "id": "c233ad0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "7bde0ad5-b1a5-4cf2-9d02-496822cff217"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa1facd8310>]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(history.history['loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "185148c1",
      "metadata": {
        "id": "185148c1"
      },
      "outputs": [],
      "source": [
        "key = \"politicians do not have permission to do what needs to be done.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "5d2043df",
      "metadata": {
        "id": "5d2043df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba7a70a-2e91-41b0-e24a-2df32cb70186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 244 2417 1627 2881  529 2267 3070 1796 1627 1792 5144 1796  121 5177\n",
            "  2912    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [04:29<00:00,  4.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कि कि रहा कि न है करना , प्रकार वह होते हैं | हैं सकती है चाहिए । चाहिए । चाहिए चाहिए . . . . है । । । . . . . है । . . . । । . । . । । । । । । । ।\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "bpe_obj = BPE(codes = codec_file)\n",
        "key = bpe_obj.process_line(key)\n",
        "\n",
        "encoding = dataset.primary_vectorizer(key).numpy().reshape(1, 64)\n",
        "print(encoding)\n",
        "decoding = np.zeros(shape=(1, 64))\n",
        "\n",
        "for i in tqdm(range(64)):\n",
        "    decoding[0, i] = tf.argmax(model((decoding, encoding)), -1)\n",
        "\n",
        "decoding = tf.convert_to_tensor(decoding)\n",
        "translation = dataset.convert_to_secondary(decoding)\n",
        "print(\"\\n\")\n",
        "print(translation.replace('@@ ', ''))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = \"This percentage is even greater than the percentage in India.\""
      ],
      "metadata": {
        "id": "l1QOsuYDe6lT"
      },
      "id": "l1QOsuYDe6lT",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_obj = BPE(codes = codec_file)\n",
        "key = bpe_obj.process_line(key)\n",
        "\n",
        "encoding = dataset.primary_vectorizer(key).numpy().reshape(1, 64)\n",
        "print(encoding)\n",
        "decoding = np.zeros(shape=(1, 64))\n",
        "\n",
        "for i in tqdm(range(64)):\n",
        "    decoding[0, i] = tf.argmax(model((decoding, encoding)), -1)\n",
        "\n",
        "decoding = tf.convert_to_tensor(decoding)\n",
        "translation = dataset.convert_to_secondary(decoding)\n",
        "print(\"\\n\")\n",
        "print(translation.replace('@@ ', ''))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kg6Sbqz_fRpi",
        "outputId": "946d13ef-84e2-4dd3-c61e-e112dcab73d2"
      },
      "id": "Kg6Sbqz_fRpi",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 990 1923 4688 1156 5134 2208  968  967 1923 4688 1538 2662    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [04:26<00:00,  4.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है। है। है। है। है। है। है। है। है। है। है। है। है। है। है। है। है। है। की है। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं।\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = \"humans destroyed the commons that they depended on.\""
      ],
      "metadata": {
        "id": "kiT9eco3fTsi"
      },
      "id": "kiT9eco3fTsi",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_obj = BPE(codes = codec_file)\n",
        "key = bpe_obj.process_line(key)\n",
        "\n",
        "encoding = dataset.primary_vectorizer(key).numpy().reshape(1, 64)\n",
        "print(encoding)\n",
        "decoding = np.zeros(shape=(1, 64))\n",
        "\n",
        "for i in tqdm(range(64)):\n",
        "    decoding[0, i] = tf.argmax(model((decoding, encoding)), -1)\n",
        "\n",
        "decoding = tf.convert_to_tensor(decoding)\n",
        "translation = dataset.convert_to_secondary(decoding)\n",
        "print(\"\\n\")\n",
        "print(translation.replace('@@ ', ''))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvc3uhiFfbux",
        "outputId": "7a64f6a7-04f6-4cca-fcb4-a6cc5ec46079"
      },
      "id": "xvc3uhiFfbux",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4478 2417 4946 1744  967 4787 2507  154 1675 3107 5107  230    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [04:10<00:00,  3.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "मानवों ने उन ही साझे संसाधनों को नष्ट किया जिन पर वो आधारित थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे। थे।\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = \"Sanskrit is world's oldest language of vedas.\""
      ],
      "metadata": {
        "id": "19e4Q5wOfdux"
      },
      "id": "19e4Q5wOfdux",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_obj = BPE(codes = codec_file)\n",
        "key = bpe_obj.process_line(key)\n",
        "\n",
        "encoding = dataset.primary_vectorizer(key).numpy().reshape(1, 64)\n",
        "print(encoding)\n",
        "decoding = np.zeros(shape=(1, 64))\n",
        "\n",
        "for i in tqdm(range(64)):\n",
        "    decoding[0, i] = tf.argmax(model((decoding, encoding)), -1)\n",
        "\n",
        "decoding = tf.convert_to_tensor(decoding)\n",
        "translation = dataset.convert_to_secondary(decoding)\n",
        "print(\"\\n\")\n",
        "print(translation.replace('@@ ', ''))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAJCtFvofmTh",
        "outputId": "69a6a8fd-4401-48a0-8302-2ee2571b37ab"
      },
      "id": "FAJCtFvofmTh",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3872 1156 3169 2997 3332 2890  243 1585  839  928    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [04:15<00:00,  4.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "संस्कृत वका सेसेसेसेसेसेलिए में करें ही... ... ... ... ... ... ... ... ... हीहै। | है। है। है। नाम है। शामिल हैं है। है। है। है। शामिल है. है। है। है। है। है। है। है। है। है। है। है। है। है। है। है। है। है। है। है। है। है। | है। | रहे रहे\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = \"In Indian culture, Holy texts have a special importance, and the Purans are among the most important of all the holy texts.\""
      ],
      "metadata": {
        "id": "Msr3ZFo8jAHw"
      },
      "id": "Msr3ZFo8jAHw",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_obj = BPE(codes = codec_file)\n",
        "key = bpe_obj.process_line(key)\n",
        "\n",
        "encoding = dataset.primary_vectorizer(key).numpy().reshape(1, 64)\n",
        "print(encoding)\n",
        "decoding = np.zeros(shape=(1, 64))\n",
        "\n",
        "for i in tqdm(range(64)):\n",
        "    decoding[0, i] = tf.argmax(model((decoding, encoding)), -1)\n",
        "\n",
        "decoding = tf.convert_to_tensor(decoding)\n",
        "translation = dataset.convert_to_secondary(decoding)\n",
        "print(\"\\n\")\n",
        "print(translation.replace('@@ ', ''))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MTaAY6FjGFQ",
        "outputId": "bc80405f-a14a-4fae-de90-94a9cc8606aa"
      },
      "id": "-MTaAY6FjGFQ",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3165 2661 1595  231 4623 3252 4948 4396  529 2833 1649 1973 2585  877\n",
            "   967 4259 2417 3754 3126  967 3193 3609 1585 1697  967 3870 4948 1277\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [04:29<00:00,  4.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "भारतीय जीवन-धारा में जिन ग्रन्थों का महत्वपूर्ण स्थान है उनमें पुराण भक्ति-ग्रंथों के रूप में बहुत महत्वपूर्ण माने जाते हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं।\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = \"In Indian culture, Holy texts are written in Sanskrit language in greater percentage\""
      ],
      "metadata": {
        "id": "GbRYJnGujH8R"
      },
      "id": "GbRYJnGujH8R",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_obj = BPE(codes = codec_file)\n",
        "key = bpe_obj.process_line(key)\n",
        "\n",
        "encoding = dataset.primary_vectorizer(key).numpy().reshape(1, 64)\n",
        "print(encoding)\n",
        "decoding = np.zeros(shape=(1, 64))\n",
        "\n",
        "for i in tqdm(range(64)):\n",
        "    decoding[0, i] = tf.argmax(model((decoding, encoding)), -1)\n",
        "\n",
        "decoding = tf.convert_to_tensor(decoding)\n",
        "translation = dataset.convert_to_secondary(decoding)\n",
        "print(\"\\n\")\n",
        "print(translation.replace('@@ ', ''))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SCuOi3tjWLA",
        "outputId": "b6b789e9-bf78-47cc-88d0-a7a739d559c9"
      },
      "id": "2SCuOi3tjWLA",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3165 2661 1595  231 4623 3252 4948 4396 3754 4395 1538 3872  243 1538\n",
            "  2208 1923 4688    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [04:16<00:00,  4.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "भारतीय आयु संस्कृत बिप्राप्त की के तततहो ही | होना है के दो तथा बी बी दो दो दो दो है, | है . और . . . और . और पर . और . . भी पर . पर करने . ) है . . . . . . . . . . . . . . . .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = \"India and China are countries in Asia\""
      ],
      "metadata": {
        "id": "IqT2zstxjYnx"
      },
      "id": "IqT2zstxjYnx",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_obj = BPE(codes = codec_file)\n",
        "key = bpe_obj.process_line(key)\n",
        "\n",
        "encoding = dataset.primary_vectorizer(key).numpy().reshape(1, 64)\n",
        "print(encoding)\n",
        "decoding = np.zeros(shape=(1, 64))\n",
        "\n",
        "for i in tqdm(range(64)):\n",
        "    decoding[0, i] = tf.argmax(model((decoding, encoding)), -1)\n",
        "\n",
        "decoding = tf.convert_to_tensor(decoding)\n",
        "translation = dataset.convert_to_secondary(decoding)\n",
        "print(\"\\n\")\n",
        "print(translation.replace('@@ ', ''))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTH6bgB_jdxw",
        "outputId": "cf6e0e8b-cbb4-4074-d534-d3ef1f136210"
      },
      "id": "CTH6bgB_jdxw",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2844  877  399 3754  690 1538 1373 2833    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [04:07<00:00,  3.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "और और याही ों एक एक मयामममजुभर भर है, है, है हैं हूँ, हूँ, । । कोहै। सहेकोहै। है। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। हैं। । । । । । । । ।\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = \"Ancient Sanskrit literature is extremely old, vast and diverse\""
      ],
      "metadata": {
        "id": "r3JTWpDjm4F-"
      },
      "id": "r3JTWpDjm4F-",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_obj = BPE(codes = codec_file)\n",
        "key = bpe_obj.process_line(key)\n",
        "\n",
        "encoding = dataset.primary_vectorizer(key).numpy().reshape(1, 64)\n",
        "print(encoding)\n",
        "decoding = np.zeros(shape=(1, 64))\n",
        "\n",
        "for i in tqdm(range(64)):\n",
        "    decoding[0, i] = tf.argmax(model((decoding, encoding)), -1)\n",
        "\n",
        "decoding = tf.convert_to_tensor(decoding)\n",
        "translation = dataset.convert_to_secondary(decoding)\n",
        "print(\"\\n\")\n",
        "print(translation.replace('@@ ', ''))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2GThlJVnb5_",
        "outputId": "c76884de-6264-4836-8b10-fa2d1eb10a81"
      },
      "id": "M2GThlJVnb5_",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 118 4644 3872  368 1156 1619  533    6 3003  877 1903 4402    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [04:20<00:00,  4.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "संस्कृत का प्राचीन साहित्य अत्यन्त प्राचीन विशाल और विविधतापूर्ण है। है। है। है। है। है। है। है। में है। है। है। है। है। है। है. है. की है. दीहै. है. है. टटहै. है. में है. है. हुए ते है. से है. है. है. है. है. है. है. है. है. है. है. है. है. है. है. है. है. है.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = \"Ancient Sanskrit literature is extremely old religious creations in Hindi.\""
      ],
      "metadata": {
        "id": "gY7f1iqzneB_"
      },
      "id": "gY7f1iqzneB_",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_obj = BPE(codes = codec_file)\n",
        "key = bpe_obj.process_line(key)\n",
        "\n",
        "encoding = dataset.primary_vectorizer(key).numpy().reshape(1, 64)\n",
        "print(encoding)\n",
        "decoding = np.zeros(shape=(1, 64))\n",
        "\n",
        "for i in tqdm(range(64)):\n",
        "    decoding[0, i] = tf.argmax(model((decoding, encoding)), -1)\n",
        "\n",
        "decoding = tf.convert_to_tensor(decoding)\n",
        "translation = dataset.convert_to_secondary(decoding)\n",
        "print(\"\\n\")\n",
        "print(translation.replace('@@ ', ''))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDdT2Loan4kw",
        "outputId": "0e9a957a-14df-4b09-b0d7-2729787b7264"
      },
      "id": "YDdT2Loan4kw",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 118 4644 3872  368 1156 1619 1222 4730 4923 1118 1538 1212 2062    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [04:32<00:00,  4.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "संस्कृत एक दूसरा पूर्ण सहायता सहायता सम्का बोउपयोग उपयोग का ों स्ट है। का है। है। है। है। है। है। है। है। है। है। की होती है। ध्यान का है| है| है| है| है| है| है| है| है| है| है| है| है| है| है| है| है| है| है| है| है। है। है। है। है। है। है। है। है। है। है। है। है।\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}